{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7Hu0zHshSKI"
   },
   "source": [
    "# ğŸ§ª **Swarm** Demo with Pre-Built Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvwQFCBShX4O"
   },
   "source": [
    "In this demo, weâ€™re going to explore how to build a simple multi-agent system using LangGraphâ€™s Swarm library.\n",
    "We'll create two AI agents with different roles who can hand off tasks to each other dynamically â€” without using any tools.\n",
    "This is perfect for showcasing agent collaboration, decision-making, and handoff logic using just prompts and messages.\n",
    "\n",
    "By the end of this notebook, youâ€™ll understand how to:\n",
    "\n",
    "* Create agents using `create_react_agent`\n",
    "\n",
    "* Connect them in a swarm using create_swarm\n",
    "\n",
    "* Enable dynamic handoffs between agents\n",
    "\n",
    "* Run a conversation where agents decide who should respond next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8ZZgv44hsXQ"
   },
   "source": [
    "## ğŸ“¦ Step 1: Install the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXw-PhI5jBD2"
   },
   "source": [
    "Weâ€™re making sure LangChain, LangGraph, and the OpenAI wrapper are on the latest version so every API we call later is available. The -U flag upgrades anything thatâ€™s already installed. We will also need the laggraph-swarm library on its latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14135,
     "status": "ok",
     "timestamp": 1754384159976,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "PUrUBHdZICMD",
    "outputId": "b7ff825d-2636-4435-fea2-55bf397b5144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.3.27\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langgraph==0.6.4\n",
      "  Downloading langgraph-0.6.4-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-openai==0.3.28\n",
      "  Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langgraph-swarm==0.0.14\n",
      "  Downloading langgraph_swarm-0.0.14-py3-none-any.whl (10 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain==0.3.27)\n",
      "  Downloading langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain==0.3.27)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain==0.3.27)\n",
      "  Downloading langsmith-0.4.42-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m401.9/401.9 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3.0.0,>=2.7.4 (from langchain==0.3.27)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting SQLAlchemy<3,>=1.4 (from langchain==0.3.27)\n",
      "  Downloading sqlalchemy-2.0.44-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain==0.3.27) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain==0.3.27) (6.0.1)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph==0.6.4)\n",
      "  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph==0.6.4)\n",
      "  Downloading langgraph_prebuilt-0.6.5-py3-none-any.whl (28 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph==0.6.4)\n",
      "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash>=3.5.0 (from langgraph==0.6.4)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai<2.0.0,>=1.86.0 (from langchain-openai==0.3.28)\n",
      "  Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain-openai==0.3.28)\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.7.0 (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging<26.0.0,>=23.2.0 (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27)\n",
      "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.6.4)\n",
      "  Downloading ormsgpack-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.25.2 (from langgraph-sdk<0.3.0,>=0.2.0->langgraph==0.6.4)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.0->langgraph==0.6.4)\n",
      "  Downloading orjson-3.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m136.2/136.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain==0.3.27)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain==0.3.27)\n",
      "  Downloading zstandard-0.25.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (4.0.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28)\n",
      "  Downloading jiter-0.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m364.6/364.6 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai==0.3.28) (4.66.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27)\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain==0.3.27)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.27) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.27) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.27) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.27) (2023.7.22)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain==0.3.27)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (587 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m587.7/587.7 kB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai==0.3.28)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph==0.6.4)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph==0.6.4)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (2.1)\n",
      "Installing collected packages: zstandard, xxhash, typing-extensions, tenacity, regex, packaging, ormsgpack, orjson, jsonpatch, jiter, h11, greenlet, distro, annotated-types, typing-inspection, tiktoken, SQLAlchemy, requests-toolbelt, pydantic-core, httpcore, pydantic, httpx, openai, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph-prebuilt, langchain, langgraph, langgraph-swarm\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.19.0\n",
      "    Uninstalling zstandard-0.19.0:\n",
      "      Successfully uninstalled zstandard-0.19.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.27.9\n",
      "    Uninstalling openai-0.27.9:\n",
      "      Successfully uninstalled openai-0.27.9\n",
      "Successfully installed SQLAlchemy-2.0.44 annotated-types-0.7.0 distro-1.9.0 greenlet-3.2.4 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.12.0 jsonpatch-1.33 langchain-0.3.27 langchain-core-0.3.79 langchain-openai-0.3.28 langchain-text-splitters-0.3.11 langgraph-0.6.4 langgraph-checkpoint-2.1.2 langgraph-prebuilt-0.6.5 langgraph-sdk-0.2.9 langgraph-swarm-0.0.14 langsmith-0.4.42 openai-1.109.1 orjson-3.11.4 ormsgpack-1.12.0 packaging-25.0 pydantic-2.12.4 pydantic-core-2.41.5 regex-2025.11.3 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.12.0 typing-extensions-4.15.0 typing-inspection-0.4.2 xxhash-3.6.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \\\n",
    "    langchain==0.3.27 \\\n",
    "    langgraph==0.6.4 \\\n",
    "    langchain-openai==0.3.28 \\\n",
    "    langgraph-swarm==0.0.14 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqUtEHcshvmh"
   },
   "source": [
    "## ğŸ§  Step 2: Import libraries\n",
    "\n",
    "In this step, we're bringing in all the tools we need to build our multi-agent system.\n",
    "\n",
    "ğŸ§© What weâ€™re importing:\n",
    "\n",
    "* `AzureChatOpenAI` â€“ This is the actual language model that our agents will use to think, reason, and generate responses.\n",
    "\n",
    "* `InMemorySaver` â€“ A simple way to store conversation history in memory. In this case, we will use short-term memory so the system remembers past messages during the session.\n",
    "\n",
    "* `create_react_agent` â€“ This is a handy function that lets us quickly create an agent with tools and logic baked in.\n",
    "\n",
    "* `create_handoff_tool` â€“ This allows one agent to pass the conversation to another agent when itâ€™s no longer the right one to handle the task.\n",
    "\n",
    "* `create_swarm` â€“ This function brings all the agents together into a Swarm, where they can work together and hand off tasks as needed. Itâ€™s what weâ€™ll use to build the collaborative environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5442,
     "status": "ok",
     "timestamp": 1754384165427,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "cae59a2b"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xQf7m8whzEh"
   },
   "source": [
    "## ğŸ” Step 3: Set up your OpenAI API key\n",
    "\n",
    "The agent will talk to OpenAIâ€™s GPT-4o-mini model. Putting your key in an environment variable lets every downstream cell authenticate automatically without hard-coding secrets into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1754384165431,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "blfbv9RUI1zP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AZURE_OPENAI_API_KEY'] = '<your-key>'\n",
    "os.environ['AZURE_OPENAI_ENDPOINT'] = '<your-endpoint>'\n",
    "os.environ['OPENAI_API_VERSION'] = '2023-05-15'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5AxiGKji23H"
   },
   "source": [
    "## ğŸ§  Step 4: Load the Model\n",
    "\n",
    "`AzureChatOpenAI` gives us a convenient Python object that wraps the GPT-4o-mini API. Setting `temperature=0.7` makes answers a little more creative while still fairly reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1630,
     "status": "ok",
     "timestamp": 1754384167065,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "3tgeOBUGJDgp"
   },
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o-mini\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8dLVd1EjRcb"
   },
   "source": [
    "## ğŸ‘¥ Step 5: Create Agent 1 â€“ the task-focused agent\n",
    "Agent 1 is focused on structured queries or task-specific help.\n",
    "If it receives a message that feels more conversational, it will hand the task off to Agent 2.\n",
    "We use create_handoff_tool() to make this handoff behavior possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1754384167082,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "UOH8wFZugcaT"
   },
   "outputs": [],
   "source": [
    "# Agent 1: Handles task-related queries, can hand off to Agent 2\n",
    "agent_1 = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[\n",
    "        create_handoff_tool(\n",
    "            agent_name=\"Agent2\",\n",
    "            description=\"For more casual or open-ended conversation.\"\n",
    "        ),\n",
    "    ],\n",
    "    prompt=\"You are Agent 1. You are focused on helping the user with structured or task-specific queries. If the message is conversational or outside your scope, hand it off to Agent 2.\",\n",
    "    name=\"Agent1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDtPLNu9kUYc"
   },
   "source": [
    "## ğŸ’¬ Step 6: Create Agent 2 â€“ the casual conversation agent\n",
    "Agent 2 handles open-ended and general conversation.\n",
    "If the user starts asking for help with a task or something more structured, Agent 2 will hand it back to Agent 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1754384167133,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "xra3EVhAgf5i"
   },
   "outputs": [],
   "source": [
    "# Agent 2: Handles open-ended or casual conversation, can hand off to Agent 1\n",
    "agent_2 = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[\n",
    "        create_handoff_tool(\n",
    "            agent_name=\"Agent1\",\n",
    "            description=\"For task-related or structured queries.\"\n",
    "        ),\n",
    "    ],\n",
    "    prompt=\"You are Agent 2. You specialize in casual conversation and open-ended questions. If the user needs structured help, hand it off to Agent 1.\",\n",
    "    name=\"Agent2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ6yqD4Vkfe6"
   },
   "source": [
    "# ğŸ’¾ Step 7: Set up memory checkpointing\n",
    "LangGraph uses checkpoints to keep track of the conversation across turns.\n",
    "We'll use an in-memory saver here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1754384167148,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "kOk01CfNgj0o"
   },
   "outputs": [],
   "source": [
    "# Use in-memory checkpointing\n",
    "checkpointer = InMemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFiQwbZxkp4F"
   },
   "source": [
    "## ğŸ¤– Step 8: Create the Swarm\n",
    "Now that weâ€™ve defined both agents, weâ€™ll bring them together into a Swarm.\n",
    "\n",
    "We also specify which agent should start the conversationâ€”in this case, weâ€™re starting with Agent 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1754384167169,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "9oHvuy0PgpIL"
   },
   "outputs": [],
   "source": [
    "# Create the swarm\n",
    "workflow = create_swarm(\n",
    "    agents=[agent_1, agent_2],\n",
    "    default_active_agent=\"Agent2\",  # Starting with Agent 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C44pbXx1k3IE"
   },
   "source": [
    "âœ… At this point, weâ€™ve defined the structure of our agent teamâ€”but itâ€™s not running yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WX2eJIzk54Y"
   },
   "source": [
    "## ğŸ› ï¸ Step 9: Compile the Swarm\n",
    "Before we can use our swarm in a live conversation, we need to compile it.\n",
    "Compiling finalizes the graph and prepares it for interaction.\n",
    "We also plug in our checkpointer, so the system remembers what was said in this thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1754384167183,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "18evAnDDgsgy"
   },
   "outputs": [],
   "source": [
    "# Compile the app\n",
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzAIMj1Nk_3G"
   },
   "source": [
    "## ğŸ§µ Step 10: Set the thread ID for this conversation\n",
    "Each conversation needs a thread_id to track the message history.\n",
    "We'll keep it simple and use \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1754384167205,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "sYsSsCSqgvUy"
   },
   "outputs": [],
   "source": [
    "# Set up conversation config\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvg843m_lHlj"
   },
   "source": [
    "## ğŸ’¬ Step 11: Start the conversation â€“ a casual message\n",
    "Letâ€™s simulate a user message to start a casual chat.\n",
    "Since it's not a task, Agent 2 should handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1754384168431,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "L38jTsn5gzy0",
    "outputId": "3829fcf2-5b93-4228-eebb-7295556ca820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hey, I just wanted to chat a bit.', additional_kwargs={}, response_metadata={}, id='f59f0a6a-2ada-4123-b759-9cac5a6ff555'), AIMessage(content=\"Of course! I'm here for a chat. What's on your mind?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 81, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-Cbob8ektcvi96Yb3KtDD20Ik8Tl27', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, name='Agent2', id='run--9e62e8e8-326b-448e-acd9-ebc7cf481b20-0', usage_metadata={'input_tokens': 81, 'output_tokens': 16, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "# User opens the conversation\n",
    "turn_1 = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hey, I just wanted to chat a bit.\"}]},\n",
    "    config,\n",
    ")\n",
    "print(turn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1754384168453,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "D8MSZLhBrVB-",
    "outputId": "eb3bc39b-e332-4e6f-d3a5-28df3996b5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Of course! I'm here for a chat. What's on your mind?\n"
     ]
    }
   ],
   "source": [
    "first_response = turn_1[\"messages\"][-1]\n",
    "print(\"Agent:\", first_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PFi7r8glQGW"
   },
   "source": [
    "## ğŸ”„ Step 12: Follow-up â€“ a task-based request\n",
    "Now the user switches gears and asks for help with a task.\n",
    "Agent 2 should recognize this and hand off the conversation to Agent 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1776,
     "status": "ok",
     "timestamp": 1754384170234,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "u2Kw9aIqgOPT",
    "outputId": "59f401f6-0bb5-4c04-bb08-c75c58289cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hey, I just wanted to chat a bit.', additional_kwargs={}, response_metadata={}, id='f59f0a6a-2ada-4123-b759-9cac5a6ff555'), AIMessage(content=\"Of course! I'm here for a chat. What's on your mind?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 81, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-Cbob8ektcvi96Yb3KtDD20Ik8Tl27', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, name='Agent2', id='run--9e62e8e8-326b-448e-acd9-ebc7cf481b20-0', usage_metadata={'input_tokens': 81, 'output_tokens': 16, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='Actually, can you help me write a short email?', additional_kwargs={}, response_metadata={}, id='aca39247-22f2-4f3e-b491-51d1eedceadd'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IZVabrpsP7tOVTROEmr6w7EP', 'function': {'arguments': '{}', 'name': 'transfer_to_agent1'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 117, 'total_tokens': 130, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CbocB61l93IA4rOzA1r2jgpxL5fRf', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, name='Agent2', id='run--de338894-e9e1-4b84-a6e7-b0a136fd3011-0', tool_calls=[{'name': 'transfer_to_agent1', 'args': {}, 'id': 'call_IZVabrpsP7tOVTROEmr6w7EP', 'type': 'tool_call'}], usage_metadata={'input_tokens': 117, 'output_tokens': 13, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully transferred to Agent1', name='transfer_to_agent1', id='a54f02e5-98ac-4fb1-a991-db72b7b6a84d', tool_call_id='call_IZVabrpsP7tOVTROEmr6w7EP'), AIMessage(content='Sure! What would you like the email to say, and who is it intended for?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 155, 'total_tokens': 175, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CbocC3BFx5cJyOh6O3kziC22i8nm2', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, name='Agent1', id='run--d7587e32-b3c9-43ef-bd1c-4e9f49e94eb4-0', usage_metadata={'input_tokens': 155, 'output_tokens': 20, 'total_tokens': 175, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})], 'active_agent': 'Agent1'}\n"
     ]
    }
   ],
   "source": [
    "# User asks a task-like question\n",
    "turn_2 = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Actually, can you help me write a short email?\"}]},\n",
    "    config,\n",
    ")\n",
    "print(turn_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1754384170257,
     "user": {
      "displayName": "Ana RE",
      "userId": "12917217046164012432"
     },
     "user_tz": -60
    },
    "id": "IjPoKsfSL9E3",
    "outputId": "bfd9ec89-25f7-4276-ebaa-fb08a582bba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Sure! What would you like the email to say, and who is it intended for?\n"
     ]
    }
   ],
   "source": [
    "second_response = turn_2[\"messages\"][-1]\n",
    "print(\"Agent:\", second_response.content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNb3Sa9Yjn9xB/oesH3Wx3W",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
